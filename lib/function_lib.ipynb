{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"KUpIMU8YWrwr"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"698fc86b"},"outputs":[],"source":["import numpy as np\n","import pandas as pd \n","import matplotlib.pyplot as plt\n","import seaborn as sns"]},{"cell_type":"markdown","metadata":{"id":"881b352c"},"source":["def trực quan hoá dữ liệu dự báo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"63855ad2"},"outputs":[],"source":["def Visualize_model_reg(y, y_pred):\n","    plt.figure(figsize=(12,6))\n","    plt.subplot(1,2,1)\n","    plt.scatter(y_pred, y)\n","    plt.xlabel('Model Prediction')\n","    plt.ylabel('True Vale')\n","    plt.plot([0, np.max(y) + 2*np.min(y)], [0, np.max(y) + 2*np.min(y)], '-', color = 'r')\n","    plt.subplot(1,2,2)\n","    sns.distplot(y, hist=False, color='r', label='True Value')\n","    sns.distplot(y_pred, hist=False, color='b', label='Model Prediction', axlabel='Distribution')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"037df6f8"},"outputs":[],"source":["def Visualize_model_reg2(y_train, y_test, y_train_hat, y_test_hat):\n","    plt.figure(figsize=(10,5))\n","    plt.subplot(1, 2, 1)\n","    ax1 = sns.distplot(y_train, hist=False, color=\"b\", label='Train Actual')\n","    sns.distplot(y_train_hat, hist=False, color=\"r\", label='Train Predict', ax=ax1)\n","    plt.subplot(1,2,2)\n","    ax2 = sns.distplot(y_test, hist=False, color=\"b\", label='Test Actual')\n","    sns.distplot(y_test_hat, hist=False, color=\"r\", label='Test Predict', ax=ax2)\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"147db1ce"},"source":["def tính toán so sánh kết quả dự báo và dữ liệu thật"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"06c01df6"},"outputs":[],"source":["from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","def Static_score_model_reg(y, y_pred):\n","    r2 = r2_score(y, y_pred)\n","    mse = mean_squared_error(y, y_pred)\n","    mae = mean_absolute_error(y, y_pred)\n","    return r2, mse, mae"]},{"cell_type":"markdown","metadata":{"id":"9eeea569"},"source":["Ghi model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ea263697"},"outputs":[],"source":["# lưu model và load model\n","# Ap dung duoc voi ca model dung trong feature engineering va model du doan cuoi cung\n","def Save_Object(obj, filename):\n","    import pickle\n","    with open(filename, 'wb') as file:\n","        pickle.dump(obj, file)\n","    return\n","def Load_Object(filename):\n","    import pickle\n","    with open(filename, 'rb') as file:\n","        obj = pickle.load(file)\n","    return obj"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6041,"status":"ok","timestamp":1655576496636,"user":{"displayName":"Huong Bui","userId":"06239643069762170294"},"user_tz":-420},"id":"2J8cr5Xr4Jsq","outputId":"0ab2609c-d91e-47ae-da91-0e8fba07a0f9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting https://github.com/ydataai/pandas-profiling/archive/master.zip\n","  Using cached https://github.com/ydataai/pandas-profiling/archive/master.zip (21.8 MB)\n","Requirement already satisfied: joblib~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==3.2.0) (1.1.0)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==3.2.0) (1.7.3)\n","Requirement already satisfied: pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==3.2.0) (1.3.5)\n","Requirement already satisfied: matplotlib>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==3.2.0) (3.2.2)\n","Requirement already satisfied: pydantic>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==3.2.0) (1.8.2)\n","Requirement already satisfied: PyYAML>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==3.2.0) (6.0)\n","Requirement already satisfied: jinja2>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==3.2.0) (2.11.3)\n","Requirement already satisfied: visions[type_image_path]==0.7.5 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==3.2.0) (0.7.5)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==3.2.0) (1.21.6)\n","Requirement already satisfied: htmlmin>=0.1.12 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==3.2.0) (0.1.12)\n","Requirement already satisfied: missingno>=0.4.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==3.2.0) (0.5.1)\n","Requirement already satisfied: phik>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==3.2.0) (0.12.2)\n","Requirement already satisfied: tangled-up-in-unicode==0.2.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==3.2.0) (0.2.0)\n","Requirement already satisfied: requests>=2.24.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==3.2.0) (2.28.0)\n","Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==3.2.0) (4.64.0)\n","Requirement already satisfied: seaborn>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==3.2.0) (0.11.2)\n","Requirement already satisfied: multimethod>=1.4 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling==3.2.0) (1.8)\n","Requirement already satisfied: networkx>=2.4 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.5->pandas-profiling==3.2.0) (2.6.3)\n","Requirement already satisfied: attrs>=19.3.0 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.5->pandas-profiling==3.2.0) (21.4.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.5->pandas-profiling==3.2.0) (7.1.2)\n","Requirement already satisfied: imagehash in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.5->pandas-profiling==3.2.0) (4.2.1)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.11.1->pandas-profiling==3.2.0) (2.0.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->pandas-profiling==3.2.0) (1.4.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->pandas-profiling==3.2.0) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->pandas-profiling==3.2.0) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->pandas-profiling==3.2.0) (2.8.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.2.0->pandas-profiling==3.2.0) (4.1.1)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3->pandas-profiling==3.2.0) (2022.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.2.0->pandas-profiling==3.2.0) (1.15.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling==3.2.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling==3.2.0) (2022.6.15)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling==3.2.0) (1.24.3)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling==3.2.0) (2.0.12)\n","Requirement already satisfied: PyWavelets in /usr/local/lib/python3.7/dist-packages (from imagehash->visions[type_image_path]==0.7.5->pandas-profiling==3.2.0) (1.3.0)\n"]}],"source":["!pip install https://github.com/ydataai/pandas-profiling/archive/master.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3436,"status":"ok","timestamp":1655576500062,"user":{"displayName":"Huong Bui","userId":"06239643069762170294"},"user_tz":-420},"id":"ynank8EA4MXb","outputId":"ae911df1-caec-48f1-ce25-bd0726dc9e71"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pmdarima in /usr/local/lib/python3.7/dist-packages (1.8.5)\n","Requirement already satisfied: Cython!=0.29.18,>=0.29 in /usr/local/lib/python3.7/dist-packages (from pmdarima) (0.29.30)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from pmdarima) (1.1.0)\n","Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from pmdarima) (1.0.2)\n","Requirement already satisfied: statsmodels!=0.12.0,>=0.11 in /usr/local/lib/python3.7/dist-packages (from pmdarima) (0.13.2)\n","Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /usr/local/lib/python3.7/dist-packages (from pmdarima) (57.4.0)\n","Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.7/dist-packages (from pmdarima) (1.21.6)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.7/dist-packages (from pmdarima) (1.7.3)\n","Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.7/dist-packages (from pmdarima) (1.3.5)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from pmdarima) (1.24.3)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.19->pmdarima) (2022.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.19->pmdarima) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.19->pmdarima) (1.15.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->pmdarima) (3.1.0)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.7/dist-packages (from statsmodels!=0.12.0,>=0.11->pmdarima) (21.3)\n","Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.7/dist-packages (from statsmodels!=0.12.0,>=0.11->pmdarima) (0.5.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=21.3->statsmodels!=0.12.0,>=0.11->pmdarima) (3.0.9)\n"]}],"source":["!pip install pmdarima"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k0h7XThd3_gF"},"outputs":[],"source":["# statistics libraries\n","import numpy as np\n","import pandas as pd\n","# from pandas_profiling import ProfileReport\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","# from mpl_toolkits.mplot3d import Axes3D\n","# from IPython.display import Image\n","from scipy.stats import itemfreq\n","from collections import Counter\n","from statsmodels.formula.api import ols\n","from scipy.stats import chi2_contingency, chi2\n","import statsmodels.api as sm\n","import scipy\n","from scipy import stats\n","import math\n","import time\n","\n","# data pre-processing libraries\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n","from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.compose import ColumnTransformer\n","# from imblearn.under_sampling import RandomUnderSampler\n","\n","# Models\n","from sklearn.linear_model import LinearRegression\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.tree import DecisionTreeRegressor\n","# from pmdarima import auto_arima\n","from sklearn import svm\n","from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor, GradientBoostingRegressor\n","# import xgboost as xgb \n","\n","# tensorflow libraries to create ANN and build model\n","# import tensorflow as tf\n","# from tensorflow import keras\n","# from tensorflow.keras import layers, Sequential, Input, Model\n","# from tensorflow.keras.layers import Dense\n","# from tensorflow.keras import Sequential\n","# from tensorflow.keras.layers import LSTM\n","# from tensorflow.keras.models import load_model\n","# from tensorflow.keras.callbacks import EarlyStopping\n","# from tensorflow.keras.utils import plot_model\n","# from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n","\n","# evaluation libraries\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve, f1_score\n","\n","# Turning parameter\n","from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import KFold\n","\n","# saving model libraries\n","from sklearn.pipeline import Pipeline\n","import pickle\n","\n","# funcsion libraries\n","import sys; \n","import os\n","import re\n","# path=os.path.abspath(os.path.join(os.getcwd(), os.pardir)) + '\\Lib'\n","# sys.path.insert(0,path)\n","# import import_ipynb\n","# from lib_evaluation_regression_model import *\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"Ycha_G9vBPYc"},"source":["#Step 2: DATA UNDERSTANDING"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7J39LBRp32T9"},"outputs":[],"source":["# create statistics function\n","def continous_feature_describe(lst_cols, df):\n","    col_ = []\n","    count_ = []\n","    min_ = []\n","    max_ = []\n","    mean_ = []\n","    std_ = []\n","    var_ = []\n","    skewness_ = []\n","    kurtosis_ = []\n","    do_lech = []\n","    do_nhon = []\n","\n","    for i in lst_cols:\n","        col_.append(i)\n","        count_.append(df[i].count())\n","        min_.append(df[i].min())\n","        max_.append(df[i].max())\n","        mean_.append(df[i].mean())\n","        std_.append(df[i].std())\n","        var_.append(df[i].var())\n","        skewness_.append(df[i].skew())\n","        kurtosis_.append(df[i].kurtosis())\n","        \n","        if df[i].skew() > 0: do_lech.append('lech phai')\n","        elif df[i].skew() < 0: do_lech.append('lech trai')\n","        else: do_lech.append('doi xung')\n","            \n","        if df[i].kurtosis() > 0: do_nhon.append('nhon hon PP chuan')\n","        elif df[i].kurtosis() < 0: do_nhon.append('bet hon PP chuan')\n","        else: do_nhon.append('tuong duong PP chuan')\n","\n","    df_describe = pd.DataFrame({'feature_name': col_,\n","                               'count': count_,\n","                               'min': min_,\n","                               'max': max_,\n","                               'mean': mean_,\n","                               'std': std_,\n","                               'var': var_,\n","                                'skewness': skewness_,\n","                                'kurtosis': kurtosis_,\n","                                'do_lech': do_lech,\n","                                'do_nhon': do_nhon\n","                               })\n","    return df_describe"]},{"cell_type":"markdown","metadata":{"id":"iKXe4OLIBsVe"},"source":["#Step 4&5: MODELING & EVALUATION/ANALYZE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HzZD9xCABq3T"},"outputs":[],"source":["# create Cross validation:\n","def Average_RMSE_Model(model, X, y, size=0.3, cv=10):\n","    import time\n","    from sklearn.model_selection import train_test_split    \n","    train_rmse=[]\n","    train_score=[]\n","    test_rmse=[]\n","    test_score=[]\n","    \n","    duration=[]\n","    for n in range(1, cv+1):                \n","        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=size)                                             \n","        \n","        start=time.time()        \n","        model.fit(X_train, y_train)\n","        end=time.time()\n","        \n","        train_rmse.append(mean_squared_error(y_true=y_train, y_pred=model.predict(X_train), squared=False))\n","        test_rmse.append(mean_squared_error(y_true=y_test, y_pred=model.predict(X_test), squared=False))\n","        train_score.append(model.score(X_train, y_train))\n","        test_score.append(model.score(X_test, y_test))\n","\n","        duration.append((end-start)*1000)\n","        \n","    return np.mean(train_rmse), np.mean(train_score), np.mean(test_rmse), np.mean(test_score), np.mean(duration) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J70bGn3s4SlK"},"outputs":[],"source":["def Save_Object(obj, filename):\n","    with open(filename, 'wb') as file:\n","        pickle.dump(obj, file)\n","    return\n","def Load_Object(filename):\n","    with open(filename, 'rb') as file:\n","        obj = pickle.load(file)\n","    return obj"]},{"cell_type":"markdown","metadata":{"id":"LNhqjBlQDkOP"},"source":["# Pipeline - Project 1"]},{"cell_type":"markdown","metadata":{"id":"rL0xpd0whQTp"},"source":["## Problem 1"]},{"cell_type":"markdown","metadata":{"id":"5xSjeykx1Q8n"},"source":["### Create def"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tWq9YRLst2N4"},"outputs":[],"source":["def data_cleaning(data):\n","    df = data.copy()\n","    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n","    # drop duplicates\n","    df = df.drop_duplicates() \n","    # change datatype\n","    df['Date'] = pd.to_datetime(df['Date']) \n","    # change column names\n","    df.columns = df.columns.str.replace(\" \",\"_\")\n","    df.columns = ['Date', 'AveragePrice', 'Total_Volume', 'PLU_4046','PLU_4225','PLU_4770',\n","          'Total_Bags', 'Small_Bags', 'Large_Bags', 'XLarge_Bags', 'type', 'year',\n","          'region']\n","    # remove some region: \n","    df = df.loc[df['region'] != \"TotalUS\",:]\n","    return df"]},{"cell_type":"markdown","metadata":{"id":"FVTwGtjv3F3U"},"source":["### Create fit_transform function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vCfhdi9-_Om9"},"outputs":[],"source":["from sklearn.base import BaseEstimator, TransformerMixin\n","class Pre_Processing(BaseEstimator, TransformerMixin):\n","    def __init__(self):\n","        print('Khởi tạo đối tượng Pre-Processing')\n","    def fit(self, X, y=None):\n","        return self\n","    def transform(self, data, y=None):\n","        import pickle\n","        # load list categorical & continous variables\n","        lst_cate = pd.read_csv('model/lst_cate.csv').iloc[:,0].to_list()\n","        lst_cont = pd.read_csv('model/lst_cont.csv').iloc[:,0].to_list()\n","\n","        # load encoder model\n","        encoder = pickle.load(open('model/OneHotEncoder.sav','rb'))\n","\n","        # data cleaning\n","        df = data.copy()\n","        df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n","        \n","        # drop duplicates\n","        df = df.drop_duplicates() \n","\n","        ## change datatype\n","        # df['Date'] = pd.to_datetime(df['Date']) \n","\n","        ## change column names\n","        df.columns = df.columns.str.replace(\" \",\"_\")\n","        df.columns = ['Date', 'AveragePrice', 'Total_Volume', 'PLU_4046','PLU_4225','PLU_4770',\n","              'Total_Bags', 'Small_Bags', 'Large_Bags', 'XLarge_Bags', 'type', 'year',\n","              'region']\n","\n","        # remove some region: \n","        df = df.loc[df['region'] != \"TotalUS\",:]\n","\n","        # apply encoder on categorial columns\n","        lst_encode = ['type','region']\n","\n","        # arr = encoder.transform(df[lst_encode]).toarray()\n","        arr = encoder.transform(df[lst_encode])\n","\n","        cols = []\n","        n = 0\n","        for i in encoder.categories_:\n","            for j in i[0:]: \n","                t = 'oh_' + lst_encode[n] + '_' +str(j)\n","                t = t.replace('-', '_')\n","                cols.append(t)\n","            n = n+1\n","        df_oh_encode = pd.DataFrame(arr, columns=cols)\n","        \n","        # concat encoded columns to df\n","        lst_concat = lst_cont\n","        df = pd.concat([df_oh_encode.reset_index(), df.reset_index()], axis=1).iloc[:,1:]\n","\n","       \n","        return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"47RAx0QWmTzT"},"outputs":[],"source":["from sklearn.base import BaseEstimator, TransformerMixin\n","class Create_XY(BaseEstimator, TransformerMixin):\n","    def __init__(self):\n","        print('Khởi tạo đối tượng Create_XY')\n","    def fit(self, X, y=None):\n","        return self\n","    def transform(self, data, y=None):\n","        import pickle\n","        # load list categorical & continous variables\n","        lst_cate = pd.read_csv('model/lst_cate.csv').iloc[:,0].to_list()\n","        lst_cont = pd.read_csv('model/lst_cont.csv').iloc[:,0].to_list()\n","\n","        # create X, y\n","        lst_cont_fea = lst_cont\n","        # lst_cont_fea.remove('AveragePrice')\n","        df = data\n","        X = pd.concat([df.loc[:, df.columns.str.startswith(\"oh_\")], \\\n","               df[lst_cont_fea]], axis=1)\n","        \n","        y = df['AveragePrice']\n","\n","        return X, y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W4ITDcUs8ND0"},"outputs":[],"source":["from sklearn.base import BaseEstimator, TransformerMixin\n","class Data_Standardizing(BaseEstimator, TransformerMixin):\n","    def __init__(self):\n","        print('Khởi tạo đối tượng Data Standardizing')\n","    def fit(self, X, y=None):\n","        return self\n","    def transform(self, data, y=None):\n","        import pickle\n","        # load scaler model\n","        scaler = pickle.load(open('model/Problem1_RobustScaler.sav','rb'))\n","\n","        # transform data\n","        lst_cont = pd.read_csv('model/lst_cont.csv').iloc[:,0].to_list()\n","        lst_cont_fea = lst_cont\n","        lst_cont_fea.remove('AveragePrice')   \n","\n","        X = data.copy()\n","        X_before_scale = X[lst_cont_fea]\n","        X_scale = scaler.transform(X_before_scale) #--> dùng transform, không dùng fit_transform cho tập Test\n","        X_scale = pd.DataFrame(X_scale, columns=(X_before_scale.add_suffix('_scale')).columns)\n","\n","        # concat data\n","        X_new = pd.concat([X.reset_index(drop=True), X_scale], \\\n","                                axis=1)\n","        # select scaled features\n","        X_scale = pd.concat([X_new.loc[:, X_new.columns.str.startswith(\"oh_\")], \\\n","                                    X_new.loc[:, X_new.columns.str.endswith(\"_scale\")]], \\\n","                                    axis=1)\n","      \n","        return X_scale"]},{"cell_type":"markdown","metadata":{"id":"Mh0lY9GChUIq"},"source":["## Problem 2,3"]},{"cell_type":"markdown","metadata":{"id":"kYXbZFjLnJ0K"},"source":["### Create def"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KOlzhw0tXF9g"},"outputs":[],"source":["def RNN_LSTM_California_organic_forecast(data, n_head=1):\n","  # n_head = input('Input number of head:')\n","  # n_head = int(n_head)\n","  scaler = pickle.load(open('outputs/Problem2_scaler_RNN_LSTM_California_organic.sav', 'rb'))\n","  model = load_model('outputs/Problem2_RNN_LSTM_California_organic_model_one.h5')\n","\n","  df = data.copy()\n","  df_California_organic = df.loc[(df['region']=='California')&(df['type']=='organic'),['Date','AveragePrice']].sort_values(by='Date')\n","  df_California_organic['Month'] = df_California_organic['Date'].to_numpy().astype('datetime64[M]')\n","  df_California_organic['Month'] = df_California_organic['Month'].astype('datetime64[ns]')\n","  df_California_organic_groupby = df_California_organic.groupby(['Month']).agg({'AveragePrice': np.mean})\n","  df_California_organic_groupby.index = pd.to_datetime(df_California_organic_groupby.index)\n","\n","  df_California_organic_groupby.index.freq = 'MS'# frequent là month\n","  df_California_organic_groupby.index.name=\"DATE\"\n","\n","  dataframe = df_California_organic_groupby\n","  dataset_one = dataframe[['AveragePrice']]\n","  dataset_one = dataset_one.values.astype('float32')\n","\n","  for i in range(n_head):\n","    x_new = dataset_one[-3:]\n","    x_new = scaler.transform(x_new) # the last value => predict next value\n","    x_new = x_new.reshape(1,1,3)\n","\n","    y_new = model.predict(x_new)\n","    y_new = scaler.inverse_transform(y_new)\n","    dataset_one = np.vstack((dataset_one,y_new ))\n","  return dataset_one"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kH4VJ9maammP"},"outputs":[],"source":["def RNN_LSTM_California_conventional_forecast(data, n_head=1):\n","  # n_head = input('Input number of head:')\n","  # n_head = int(n_head)\n","  scaler = pickle.load(open('outputs/Problem3_scaler_RNN_LSTM_California_conventional.sav', 'rb'))\n","  model = load_model('outputs/Problem3_RNN_LSTM_California_conventional_model_one.h5')\n","\n","  df = data.copy()\n","  df_California_conventional = df.loc[(df['region']=='California')&(df['type']=='conventional'),['Date','AveragePrice']].sort_values(by='Date')\n","  df_California_conventional['Month'] = df_California_conventional['Date'].to_numpy().astype('datetime64[M]')\n","  df_California_conventional['Month'] = df_California_conventional['Month'].astype('datetime64[ns]')\n","  df_California_conventional_groupby = df_California_conventional.groupby(['Month']).agg({'AveragePrice': np.mean})\n","  df_California_conventional_groupby.index = pd.to_datetime(df_California_conventional_groupby.index)\n","\n","  df_California_conventional_groupby.index.freq = 'MS'# frequent là month\n","  df_California_conventional_groupby.index.name=\"DATE\"\n","\n","  dataframe = df_California_conventional_groupby\n","  dataset_one = dataframe[['AveragePrice']]\n","  dataset_one = dataset_one.values.astype('float32')\n","\n","  for i in range(n_head):\n","    x_new = dataset_one[-3:]\n","    x_new = scaler.transform(x_new) # the last value => predict next value\n","    x_new = x_new.reshape(1,1,3)\n","\n","    y_new = model.predict(x_new)\n","    y_new = scaler.inverse_transform(y_new)\n","    dataset_one = np.vstack((dataset_one,y_new ))\n","  return dataset_one"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"baymTUwze-1U"},"outputs":[],"source":["def FP_California_organic_forecast(df, n_head):\n","  df_California_organic = df.loc[(df['region']=='California')&(df['type']=='organic'),['Date','AveragePrice']].sort_values(by='Date')\n","  df_California_organic['Month'] = df_California_organic['Date'].to_numpy().astype('datetime64[M]')\n","  df_California_organic['Month'] = df_California_organic['Month'].astype('datetime64[ns]')\n","  df_California_organic_groupby = df_California_organic.groupby(['Month']).agg({'AveragePrice': np.mean})\n","  df_California_organic_groupby.index = pd.to_datetime(df_California_organic_groupby.index)\n","\n","  df_California_organic_groupby.index.freq = 'MS'# frequent là month\n","  df_California_organic_groupby.index.name=\"DATE\"\n","  df = df_California_organic_groupby.reset_index()  \n","  df.columns = ['ds','y']\n","\n","  m = Prophet(yearly_seasonality=True, \\\n","              daily_seasonality=False, weekly_seasonality=False) \n","  m.fit(df)\n","  future = m.make_future_dataframe(periods=n_head, freq='M') # next 5 years\n","  forecast = m.predict(future)\n","\n","  return m, forecast"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wI262Ohmd4C5"},"outputs":[],"source":["def FP_California_conventional_forecast(df, n_head):\n","  df_California_conventional = df.loc[(df['region']=='California')&(df['type']=='conventional'),['Date','AveragePrice']].sort_values(by='Date')\n","  df_California_conventional['Month'] = df_California_conventional['Date'].to_numpy().astype('datetime64[M]')\n","  df_California_conventional['Month'] = df_California_conventional['Month'].astype('datetime64[ns]')\n","  df_California_conventional_groupby = df_California_conventional.groupby(['Month']).agg({'AveragePrice': np.mean})\n","  df_California_conventional_groupby.index = pd.to_datetime(df_California_conventional_groupby.index)\n","\n","  df_California_conventional_groupby.index.freq = 'MS'# frequent là month\n","  df_California_conventional_groupby.index.name=\"DATE\"\n","  df = df_California_conventional_groupby.reset_index()  \n","  df.columns = ['ds','y']\n","\n","  m = Prophet(yearly_seasonality=True, \\\n","              daily_seasonality=False, weekly_seasonality=False) \n","  m.fit(df)\n","  future = m.make_future_dataframe(periods=n_head, freq='M') # next 5 years\n","  forecast = m.predict(future)\n","\n","  return m, forecast"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ua_65W_45_ZY"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNXx4RdF/e+2ZKl39mr4clJ","collapsed_sections":[],"name":"function_lib.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3.7.9 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"vscode":{"interpreter":{"hash":"5109d816b82be14675a6b11f8e0f0d2e80f029176ed3710d54e125caa8520dfd"}}},"nbformat":4,"nbformat_minor":0}
